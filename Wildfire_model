{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4932759,"sourceType":"datasetVersion","datasetId":2860500},{"sourceId":101819,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":85358,"modelId":109579}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport shutil\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# AS DATASET IS ALREADY ARRANGED, NO NEED OF THIS PORTION\ndef combine_datasets(original_dir, target_dir):\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    classes = ['wildfire', 'nowildfire']\n    \n    paths = {\n        'wildfire': '/kaggle/input/wildfire-prediction-dataset/train/wildfire',\n        'nowildfire': '/kaggle/input/wildfire-prediction-dataset/train/nowildfire'\n    }\n    \n    for cls in classes:\n        original_path = paths[cls]\n        target_class_dir = os.path.join(target_dir, cls)\n        \n        if not os.path.exists(target_class_dir):\n            os.makedirs(target_class_dir)\n        \n        # Copy original images\n        for filename in os.listdir(original_path):\n            source_file = os.path.join(original_path, filename)\n            shutil.copy(source_file, target_class_dir)\n\n# Use the writable output directory in Kaggle for the combined data\noriginal_dir = '/kaggle/input/wildfire-prediction-dataset/train'\ntarget_dir = '/kaggle/working/combined_data'\ncombine_datasets(original_dir, target_dir)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\n\ndef remove_corrupted_images(directory):\n    for root, dirs, files in os.walk(directory):\n        for filename in files:\n            file_path = os.path.join(root, filename)\n            try:\n                with Image.open(file_path) as img:\n                    img.verify()  # Verify that it is an image\n            except (IOError, SyntaxError) as e:\n                print(f\"Removing corrupted image: {file_path}\")\n                os.remove(file_path)\n\n# Clean both the wildfire and no wildfire directories in the working directory\nremove_corrupted_images('/kaggle/working/combined_data/wildfire')\nremove_corrupted_images('/kaggle/working/combined_data/nowildfire')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def repair_corrupted_images(directory):\n    for root, dirs, files in os.walk(directory):\n        for filename in files:\n            file_path = os.path.join(root, filename)\n            try:\n                with Image.open(file_path) as img:\n                    img.load()  # Force loading of the image data\n                    img.save(file_path)  # Save image to potentially repair any issues\n            except (IOError, SyntaxError) as e:\n                print(f\"Removing corrupted image: {file_path}\")\n                os.remove(file_path)\n\n# Repair images in the combined dataset\nrepair_corrupted_images('/kaggle/working/combined_data/wildfire')\nrepair_corrupted_images('/kaggle/working/combined_data/nowildfire')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Set up the ImageDataGenerator for data augmentation and rescaling\ndatagen = ImageDataGenerator(rescale=1./255, validation_split=0.15)\n\n# Load the training and validation data\ntrain_generator = datagen.flow_from_directory(\n    '/kaggle/working/combined_data',\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='binary',\n    subset='training'\n)\n\nvalidation_generator = datagen.flow_from_directory(\n    '/kaggle/working/combined_data',\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='binary',\n    subset='validation'\n)\n\nprint(\"Data generators are set up correctly.\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the base model\nbase_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n\n# Add new layers on top\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(128, activation='relu')(x)\npredictions = Dense(1, activation='sigmoid')(x)\n\n# Create the full model\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# Freeze the base model layers\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Train the model\nhistory = model.fit(\n    train_generator,\n    validation_data=validation_generator,\n    epochs=10\n)\n\n# Save the model\nmodel.save('/kaggle/working/wildfire_model.h5')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model\nmodel.save('wildfire_model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# AS DATASET IS ALREADY ARRANGED, NO NEED OF THIS PORTION\ndef combine_datasets(original_dir, target_dir):\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    classes = ['wildfire', 'nowildfire']\n    \n    paths = {\n        'wildfire': '/kaggle/input/wildfire-prediction-dataset/test/wildfire',\n        'nowildfire': '/kaggle/input/wildfire-prediction-dataset/test/nowildfire'\n    }\n    \n    for cls in classes:\n        original_path = paths[cls]\n        target_class_dir = os.path.join(target_dir, cls)\n        \n        if not os.path.exists(target_class_dir):\n            os.makedirs(target_class_dir)\n        \n        # Copy original images\n        for filename in os.listdir(original_path):\n            source_file = os.path.join(original_path, filename)\n            shutil.copy(source_file, target_class_dir)\n\n# Use the writable output directory in Kaggle for the combined data\noriginal_dir = '/kaggle/input/wildfire-prediction-dataset/test'\ntarget_dir = '/kaggle/working/test'\ncombine_datasets(original_dir, target_dir)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Clean both the wildfire and no wildfire directories in the working directory\nremove_corrupted_images('/kaggle/working/test/wildfire')\nremove_corrupted_images('/kaggle/working/test/nowildfire')\n\n# Repair images in the combined dataset\nrepair_corrupted_images('/kaggle/working/test/wildfire')\nrepair_corrupted_images('/kaggle/working/test/nowildfire')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# (Optional) Load and evaluate the model\ntest_generator = datagen.flow_from_directory(\n    '/kaggle/working/test',  # Adjusted for your test dataset\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='binary'\n)\n\n# Load the saved model\nloaded_model = tf.keras.models.load_model('wildfire_model.h5')\n\n# Evaluate the model\ntest_loss, test_acc = loaded_model.evaluate(test_generator)\nprint(f'Test accuracy: {test_acc:.2f}')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import shutil\n# shutil.make_archive(\"Output Files\", 'zip', '/kaggle/working/')\n# def list_files(directory):\n#     for root, dirs, files in os.walk(directory):\n#         for name in files:\n#             print(os.path.join(root, name))\n\n# list_files('/kaggle/working/')\nimport shutil\n\ndef zip_directories(base_dir, directories, output_filename):\n    temp_dir = '/kaggle/working/temp_archive'\n    os.makedirs(temp_dir, exist_ok=True)\n    \n    for directory in directories:\n        full_path = os.path.join(base_dir, directory)\n        if os.path.exists(full_path):\n            shutil.copytree(full_path, os.path.join(temp_dir, os.path.basename(full_path)))\n        else:\n            print(f\"Warning: {full_path} does not exist\")\n\n    shutil.make_archive(output_filename, 'zip', temp_dir)\n    shutil.rmtree(temp_dir)  # Clean up\n\n# Directories to include in the zip file\ndirectories_to_zip = ['test', 'combined_data']\nzip_directories('/kaggle/working', directories_to_zip, 'Output_Files')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.remove('/kaggle/working/Output_Files.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nimport numpy as np\nfrom IPython.display import display\nimport ipywidgets as widgets\nimport io\n\n# Step 1: Load the trained model\nmodel = tf.keras.models.load_model('/kaggle/input/wildfiremodel/tensorflow2/default/1/wildfire_model.h5')\n\n# Prepare the image for prediction\ndef prepare_image(image_file, target_size=(224, 224)):\n    # Load the image\n    img = load_img(image_file, target_size=target_size)\n    \n    # Convert the image to a numpy array\n    img_array = img_to_array(img)\n    \n    # Rescale the image (as done during model training)\n    img_array = img_array / 255.0\n    \n    # Expand dimensions to match the input shape of the model (batch_size, height, width, channels)\n    img_array = np.expand_dims(img_array, axis=0)\n    \n    return img_array\n\n# Make a prediction on a new image\ndef predict_image(image_file):\n    # Prepare the image\n    img_array = prepare_image(image_file)\n    \n    # Make the prediction\n    prediction = model.predict(img_array)\n    \n    # Convert prediction to class label\n    if prediction[0] > 0.5:\n        return \"Risk of Wildfire\"\n    else:\n        return \"No risk of Wildfire\"\n\n# Set up the file upload widget\nupload = widgets.FileUpload(accept='image/*', multiple=False)\n\ndef on_upload_change(change):\n    # Clear previous outputs\n    output.clear_output()\n    \n    # Process uploaded files\n    for filename, file_info in upload.value.items():\n        # Convert the file info to a BytesIO object\n        image_file = io.BytesIO(file_info['content'])\n        \n        # Make a prediction on the uploaded image\n        result = predict_image(image_file)\n        \n        with output:\n            print(f'The image is predicted as: {result}')\n\nupload.observe(on_upload_change, names='value')\ndisplay(upload)\n\n# Create an output widget to display results\noutput = widgets.Output()\ndisplay(output)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-27T06:28:48.084546Z","iopub.execute_input":"2024-08-27T06:28:48.084966Z","iopub.status.idle":"2024-08-27T06:29:05.096643Z","shell.execute_reply.started":"2024-08-27T06:28:48.084919Z","shell.execute_reply":"2024-08-27T06:29:05.095446Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"FileUpload(value={}, accept='image/*', description='Upload')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecf0e2f30a72463f9eab93c025c90cba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8504aa574c249b89c419ab6ed59cfd2"}},"metadata":{}}]}]}